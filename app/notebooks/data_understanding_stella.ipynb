{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_commas(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json_file.read()\n",
    "\n",
    "    # use regex to find and remove trailing commas at the end of arrays\n",
    "    corrected_data = re.sub(r',(?=\\s*[\\]}]\\s*[\\r\\n]*$)', '', data)\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json_file.write(corrected_data)\n",
    "\n",
    "# function to load in the json file        \n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        try:\n",
    "            data = json.load(json_file)\n",
    "            return data\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON in {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "# function to create a dataframe from all the json files.\n",
    "# trying it in batches since memory can't handle it\n",
    "def create_dataframe(directory, batch_size=10, process_all_files=False):\n",
    "    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
    "\n",
    "    # try it with the first 1000 files first to see if memory can handle it\n",
    "    if not process_all_files:\n",
    "        json_files = json_files[:1000]\n",
    "\n",
    "    if json_files:\n",
    "        # store df's for each JSON file\n",
    "        dfs = []\n",
    "\n",
    "        # process JSON files in batches\n",
    "        for i in range(0, len(json_files), batch_size):\n",
    "            batch_files = json_files[i:i + batch_size]\n",
    "\n",
    "            # process all JSON files in the batch\n",
    "            for current_json_file in batch_files:\n",
    "                file_path = os.path.join(directory, current_json_file)\n",
    "\n",
    "                remove_trailing_commas(file_path)\n",
    "\n",
    "                # load the content of the current JSON file\n",
    "                data = load_json(file_path)\n",
    "\n",
    "                if data is not None:\n",
    "                    # create a df for the current JSON file\n",
    "                    df = pd.json_normalize(data, max_level=3)\n",
    "\n",
    "                    # add new column for the file name without \".json\"\n",
    "                    df['file_name'] = os.path.splitext(current_json_file)[0]\n",
    "\n",
    "                    # append the df to the list\n",
    "                    dfs.append(df)\n",
    "\n",
    "        # concatenate all df's in the list into a single df, should be out of the loop to improve efficiency(?)\n",
    "        result_df = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"No JSON files found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_directory = r'../data/output/'\n",
    "result_df_first_1000 = create_dataframe(json_directory, batch_size=5, process_all_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result_df_first_1000 is not None:\n",
    "    print(result_df_first_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcase the unique columns\n",
    "for col in result_df_first_1000:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_row = result_df_first_1000[result_df_first_1000['buff'].notna()].iloc[0]\n",
    "\n",
    "# display row\n",
    "print(\"Row where 'event.meta.triggered_by' is not NaN:\")\n",
    "print(filtered_row)\n",
    "\n",
    "# print value in the row\n",
    "eventmetatriggered_by_value = filtered_row['buff']\n",
    "print(f\"\\nValue in the 'event.meta.triggered_by' column: {eventmetatriggered_by_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe to a csv file\n",
    "# result_df_first_1000.to_csv(r'../data/first_1000_JSON_files.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
